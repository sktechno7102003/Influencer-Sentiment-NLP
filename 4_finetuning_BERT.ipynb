{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bde31a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (4.4.2)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.11/site-packages (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: imbalanced-learn in ./.venv/lib/python3.11/site-packages (0.14.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.11/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in ./.venv/lib/python3.11/site-packages (from imbalanced-learn) (0.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install transformers datasets accelerate evaluate scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3efa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc32baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "Counter({2: 15830, 1: 13042, 0: 8277})\n",
      "\n",
      "Total samples: 37149\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      clean_comment  category  \\\n",
       "0           0   family mormon have never tried explain them t...         1   \n",
       "1           1  buddhism has very much lot compatible with chr...         1   \n",
       "2           2  seriously don say thing first all they won get...        -1   \n",
       "3           3  what you have learned yours and only yours wha...         0   \n",
       "4           4  for your own benefit you may want read living ...         1   \n",
       "\n",
       "   label  \n",
       "0      2  \n",
       "1      2  \n",
       "2      0  \n",
       "3      1  \n",
       "4      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "csv_path = r'/Users/sk/Library/Mobile Documents/com~apple~CloudDocs/3. Techno (Professional)/21. Prj2_Transformer based Comment analyzer Chrome Plugin/ Prj2_CodeFile/data/dataset.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Clean the data\n",
    "df = df.dropna(subset=['clean_comment', 'category'])\n",
    "df['clean_comment'] = df['clean_comment'].astype(str)\n",
    "\n",
    "# Keep all 3 classes: -1 (negative), 0 (neutral), 1 (positive)\n",
    "# Map categories to 0, 1, 2 for model training\n",
    "# -1 -> 0 (negative), 0 -> 1 (neutral), 1 -> 2 (positive)\n",
    "df['label'] = df['category'].map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "print(\"Original class distribution:\")\n",
    "print(Counter(df['label']))\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ed891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text to TF-IDF embeddings for ADASYN...\n",
      "TF-IDF shape: (37149, 500)\n",
      "Original class distribution: Counter({np.int64(2): 15830, np.int64(1): 13042, np.int64(0): 8277})\n",
      "\n",
      "Applying ADASYN...\n",
      "Resampled shape: (44838, 500)\n",
      "Resampled class distribution: Counter({np.int64(2): 15830, np.int64(0): 15580, np.int64(1): 13428})\n"
     ]
    }
   ],
   "source": [
    "# Apply ADASYN for handling class imbalance\n",
    "# ADASYN works on numeric features, so we'll convert text to embeddings first\n",
    "# Using TF-IDF embeddings for ADASYN (lightweight and effective)\n",
    "\n",
    "print(\"Converting text to TF-IDF embeddings for ADASYN...\")\n",
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2), min_df=2)\n",
    "X_tfidf = vectorizer.fit_transform(df['clean_comment']).toarray()\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"TF-IDF shape: {X_tfidf.shape}\")\n",
    "print(f\"Original class distribution: {Counter(y)}\")\n",
    "\n",
    "# Apply ADASYN\n",
    "print(\"\\nApplying ADASYN...\")\n",
    "adasyn = ADASYN(random_state=42, n_neighbors=5)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X_tfidf, y)\n",
    "\n",
    "print(f\"Resampled shape: {X_resampled.shape}\")\n",
    "print(f\"Resampled class distribution: {Counter(y_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b99d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping synthetic samples to original text samples...\n",
      "\n",
      "Balanced dataset shape: (44838, 2)\n",
      "Balanced class distribution:\n",
      "Counter({2: 15830, 0: 15580, 1: 13428})\n"
     ]
    }
   ],
   "source": [
    "# ADASYN generates synthetic samples, but we need actual text for BERT\n",
    "# We'll map synthetic embeddings back to original text samples using nearest neighbors\n",
    "\n",
    "print(\"Mapping synthetic samples to original text samples...\")\n",
    "\n",
    "# Find nearest neighbors for synthetic samples\n",
    "nbrs = NearestNeighbors(n_neighbors=1, metric='cosine').fit(X_tfidf)\n",
    "_, indices = nbrs.kneighbors(X_resampled)\n",
    "\n",
    "# Get the original text samples corresponding to resampled indices\n",
    "resampled_indices = indices.flatten()\n",
    "resampled_texts = df['clean_comment'].iloc[resampled_indices].values\n",
    "resampled_labels = y_resampled\n",
    "\n",
    "# Create balanced dataframe\n",
    "balanced_df = pd.DataFrame({\n",
    "    'clean_comment': resampled_texts,\n",
    "    'label': resampled_labels\n",
    "})\n",
    "\n",
    "print(f\"\\nBalanced dataset shape: {balanced_df.shape}\")\n",
    "print(\"Balanced class distribution:\")\n",
    "print(Counter(balanced_df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(balanced_df.reset_index(drop=True))\n",
    "\n",
    "# Load BERT tokenizer\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40028536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function for tokenization\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"clean_comment\"], truncation=True, padding=False)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Create data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Dataset tokenized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation, and test sets\n",
    "# Train: 80%, Validation: 10%, Test: 10%\n",
    "train_testvalid = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_testvalid['train']\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "eval_dataset = test_valid['train']\n",
    "test_dataset = test_valid['test']\n",
    "\n",
    "print(f\"Train Size: {len(train_dataset)}\")\n",
    "print(f\"Validation Size: {len(eval_dataset)}\")\n",
    "print(f\"Test Size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model for 3-class classification\n",
    "id2label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Get predicted classes\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predicted_classes)\n",
    "    f1_macro = f1_score(labels, predicted_classes, average='macro')\n",
    "    f1_weighted = f1_score(labels, predicted_classes, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": round(accuracy, 4),\n",
    "        \"f1_macro\": round(f1_macro, 4),\n",
    "        \"f1_weighted\": round(f1_weighted, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-sentiment-3class-adasyn\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    fp16=False,  # Set to True if using GPU with CUDA\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedd2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "trainer.train()\n",
    "print(\"=\"*60)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d913b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "print(\"=\"*60)\n",
    "test_results = trainer.predict(test_dataset)\n",
    "test_predictions = np.argmax(test_results.predictions, axis=1)\n",
    "test_labels = test_results.label_ids\n",
    "\n",
    "# Print detailed classification report\n",
    "target_names = ['Negative (-1)', 'Neutral (0)', 'Positive (1)']\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(test_labels, test_predictions, target_names=target_names, digits=4))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(test_labels, test_predictions):.4f}\")\n",
    "print(f\"Test F1-Macro: {f1_score(test_labels, test_predictions, average='macro'):.4f}\")\n",
    "print(f\"Test F1-Weighted: {f1_score(test_labels, test_predictions, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8253a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_save_path = \"./bert-sentiment-3class-final\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
